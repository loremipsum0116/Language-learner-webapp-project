// server/remove_duplicates_from_wordlist.js
// ìˆ˜ëŠ¥ì™„ì„±_ì˜ë‹¨ì–´ë§Œ.txtì—ì„œ ì¤‘ë³µ ë‹¨ì–´ ì œê±° (ê´„í˜¸ í¬í•¨ ë‹¨ì–´ë„ ì²˜ë¦¬)

const fs = require('fs');
const path = require('path');

const inputFile = path.join(__dirname, 'ìˆ˜ëŠ¥ì™„ì„±_ì˜ë‹¨ì–´ë§Œ.txt');
const outputFile = path.join(__dirname, 'ìˆ˜ëŠ¥ì™„ì„±_ì˜ë‹¨ì–´ë§Œ_ì¤‘ë³µì œê±°.txt');

// ê¸°ë³¸ ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜ (ê´„í˜¸ì™€ ë‚´ìš© ì œê±°)
function getBaseWord(word) {
    // ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    return word.replace(/\s*\([^)]*\)\s*/g, '').trim().toLowerCase();
}

try {
    console.log('ğŸ” Reading ìˆ˜ëŠ¥ì™„ì„±_ì˜ë‹¨ì–´ë§Œ.txt...');
    
    // íŒŒì¼ ì½ê¸°
    const content = fs.readFileSync(inputFile, 'utf8');
    const words = content.split('\n')
        .map(word => word.trim())
        .filter(word => word.length > 0); // ë¹ˆ ì¤„ ì œê±°
    
    console.log(`ğŸ“Š Total words in file: ${words.length}`);
    
    // ì¤‘ë³µ ì œê±° ì²˜ë¦¬
    const uniqueWords = [];
    const seenBaseWords = new Set();
    let duplicateCount = 0;
    
    for (const word of words) {
        const baseWord = getBaseWord(word);
        
        if (!seenBaseWords.has(baseWord)) {
            // ì²˜ìŒ ë³´ëŠ” ê¸°ë³¸ ë‹¨ì–´ì¸ ê²½ìš°
            seenBaseWords.add(baseWord);
            uniqueWords.push(word);
        } else {
            // ì¤‘ë³µëœ ê¸°ë³¸ ë‹¨ì–´ì¸ ê²½ìš°
            duplicateCount++;
            console.log(`ğŸ”„ Duplicate removed: "${word}" (base word: "${baseWord}")`);
        }
    }
    
    // ê²°ê³¼ë¥¼ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬
    uniqueWords.sort((a, b) => {
        const baseA = getBaseWord(a);
        const baseB = getBaseWord(b);
        return baseA.localeCompare(baseB);
    });
    
    console.log(`\nğŸ“ˆ Processing Summary:`);
    console.log(`   Original words: ${words.length}`);
    console.log(`   Unique words: ${uniqueWords.length}`);
    console.log(`   Duplicates removed: ${duplicateCount}`);
    
    // íŒŒì¼ë¡œ ì €ì¥ (ê° ë‹¨ì–´ë¥¼ í•œ ì¤„ì”©)
    const output = uniqueWords.join('\n');
    fs.writeFileSync(outputFile, output, 'utf8');
    
    console.log(`\nğŸ“ Sample unique words (first 20):`);
    uniqueWords.slice(0, 20).forEach((word, index) => {
        const baseWord = getBaseWord(word);
        console.log(`   ${index + 1}. ${word}${word !== baseWord ? ` (base: ${baseWord})` : ''}`);
    });
    
    if (uniqueWords.length > 20) {
        console.log(`\nğŸ“ Sample unique words (last 10):`);
        uniqueWords.slice(-10).forEach((word, index) => {
            const baseWord = getBaseWord(word);
            const actualIndex = uniqueWords.length - 9 + index;
            console.log(`   ${actualIndex}. ${word}${word !== baseWord ? ` (base: ${baseWord})` : ''}`);
        });
    }
    
    // ì œê±°ëœ ì¤‘ë³µ ì˜ˆì‹œ ì¶œë ¥
    console.log(`\nğŸ“‹ Examples of duplicates that were removed:`);
    const exampleDuplicates = [];
    const tempSeen = new Set();
    
    for (const word of words) {
        const baseWord = getBaseWord(word);
        if (tempSeen.has(baseWord)) {
            exampleDuplicates.push(`"${word}" (base: "${baseWord}")`);
            if (exampleDuplicates.length >= 10) break;
        } else {
            tempSeen.add(baseWord);
        }
    }
    
    exampleDuplicates.forEach((example, index) => {
        console.log(`   ${index + 1}. ${example}`);
    });
    
    console.log(`\nâœ… Success! Deduplicated word list saved to: ${path.basename(outputFile)}`);
    console.log(`ğŸ“ File location: ${outputFile}`);
    console.log(`ğŸ“Š Final unique words: ${uniqueWords.length}`);
    
} catch (error) {
    console.error('âŒ Error processing word list:', error.message);
    process.exit(1);
}