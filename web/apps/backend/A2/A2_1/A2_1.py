# -*- coding: utf-8 -*-
import json
import os
os.environ["GRPC_DNS_RESOLVER"] = "native"
import re
from io import BytesIO
import difflib  # âœ… ì¶”ê°€: ìœ ì‚¬ë„ ê³„ì‚°

from google.cloud import texttospeech
from google.cloud import speech  # âœ… ì¶”ê°€: STT
from pydub import AudioSegment

# âœ… ì¶”ê°€: STT ì„ê³„ì¹˜ (í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥)
STT_ACCURACY_THRESHOLD = float(os.getenv("STT_ACCURACY_THRESHOLD", "0.82"))

def sanitize_filename(name):
    name = re.sub(r'[\\/*?:"<>|]', "", name)
    return name.lower()

def split_script_by_language(script_text):
    """í•œê¸€/ì˜ì–´ í˜¼í•© í…ìŠ¤íŠ¸ë¥¼ ì–¸ì–´ë³„ë¡œ ë¶„ë¦¬."""
    segments = []
    current_lang, buf = None, ""

    for ch in script_text:
        if '\uac00' <= ch <= '\ud7af':  # í•œê¸€
            lang = 'en-US'
        elif 'a' <= ch.lower() <= 'z':
            lang = 'en-US'
        elif ch.isspace():
            lang = current_lang
        else:
            lang = current_lang

        if lang != current_lang and buf:
            segments.append((current_lang, buf))
            buf = ""
        buf += ch
        current_lang = lang

    if buf:
        segments.append((current_lang, buf))
    return segments

# âœ… ì„±ë³„ì— ë”°ë¼ ë³´ì´ìŠ¤ ë¬¶ìŒ ì •ì˜ (í•„ìš” ì‹œ ì´ë¦„ì„ í™˜ê²½ì— ë§ê²Œ ì¡°ì •)
VOICE_SETS = {
    "male": {
        "en-US": texttospeech.VoiceSelectionParams(
            language_code="en-US", name="en-US-Chirp3-HD-Charon"
        ),
        "en-US": texttospeech.VoiceSelectionParams(
            language_code="en-US", name="en-US-Chirp3-HD-Charon"
        ),
    },
    "female": {
        "en-US": texttospeech.VoiceSelectionParams(
            language_code="en-US", name="en-US-Chirp3-HD-Laomedeia"
        ),
        "en-US": texttospeech.VoiceSelectionParams(
            language_code="en-US", name="en-US-Chirp3-HD-Laomedeia"
        ),
    },
}

# ì²« í•­ëª© ì„±ë³„ ì„¤ì •: 'male'ì´ë©´ 1ë²ˆì§¸ ë‚¨/2ë²ˆì§¸ ì—¬â€¦, 'female'ì´ë©´ ë°˜ëŒ€ë¡œ ì‹œì‘
START_GENDER = "male"

def gender_for_index(idx: int, start: str = "male") -> str:
    first_is_male = (start == "male")
    return "male" if ((idx % 2 == 0) == first_is_male) else "female"

# âœ… ì¶”ê°€: í…ìŠ¤íŠ¸ ì •ê·œí™” & ìœ ì‚¬ë„
def _normalize_text(t: str) -> str:
    if not t:
        return ""
    t = t.lower()
    # í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±ë§Œ ë‚¨ê¹€
    t = re.sub(r"[^\w\uac00-\ud7af\s]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def _similarity(a: str, b: str) -> float:
    a_norm, b_norm = _normalize_text(a), _normalize_text(b)
    if not a_norm or not b_norm:
        return 0.0
    return difflib.SequenceMatcher(None, a_norm, b_norm).ratio()

# âœ… ì¶”ê°€: STTë¡œ í’ˆì§ˆ í™•ì¸
def _stt_passes(audio: AudioSegment, reference_text: str, speech_client: "speech.SpeechClient") -> tuple[bool, float, float | None, str]:
    """
    ë°˜í™˜: (í†µê³¼ì—¬ë¶€, ìœ ì‚¬ë„[0..1], í‰ê·  confidence ë˜ëŠ” None, transcript)
    """
    # wav(PCM LINEAR16)ë¡œ ë©”ëª¨ë¦¬ ë‚´ ì¸ì½”ë”©
    buf = BytesIO()
    audio.export(buf, format="wav")
    content = buf.getvalue()

    audio_rate = audio.frame_rate
    channels = audio.channels

    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=audio_rate,
        language_code="ko-KR",  # ê¸°ë³¸ í•œêµ­ì–´
        alternative_language_codes=["en-US"],  # ì˜ì–´ ë³´ì¡°
        enable_automatic_punctuation=True,
        audio_channel_count=channels,
    )
    audio_in = speech.RecognitionAudio(content=content)

    try:
        resp = speech.SpeechClient.recognize(speech_client, config=config, audio=audio_in)
    except Exception as e:
        print(f"âš ï¸ STT ì¸ì‹ ì‹¤íŒ¨: {e}")
        return (False, 0.0, None, "")

    if not resp.results:
        return (False, 0.0, None, "")

    transcript_parts = []
    confidences = []
    for r in resp.results:
        if r.alternatives:
            top = r.alternatives[0]
            transcript_parts.append(top.transcript)
            # confidenceê°€ ì—†ì„ ìˆ˜ ìˆìŒ
            if getattr(top, "confidence", None) is not None:
                confidences.append(top.confidence)

    transcript = " ".join(transcript_parts).strip()
    sim = _similarity(reference_text, transcript)
    avg_conf = sum(confidences) / len(confidences) if confidences else None

    return (sim >= STT_ACCURACY_THRESHOLD, sim, avg_conf, transcript)

def synthesize_vocab_audio(json_file_path):
    try:
        tts_client = texttospeech.TextToSpeechClient()
    except Exception as e:
        print("Google Cloud ì¸ì¦ ì‹¤íŒ¨(TTS):", e)
        return

    # âœ… ì¶”ê°€: STT í´ë¼ì´ì–¸íŠ¸
    try:
        stt_client = speech.SpeechClient()
    except Exception as e:
        print("Google Cloud ì¸ì¦ ì‹¤íŒ¨(STT):", e)
        return

    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3
    )

    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            vocab_list = json.load(f)
    except Exception as e:
        print(f"JSON íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨: {e}")
        return

    output_dir = "A1_1_audio_generated_duo"  # ì¶©ëŒ ë°©ì§€ìš© ìƒˆ í´ë”
    os.makedirs(output_dir, exist_ok=True)

    print(f"ğŸ§ '{output_dir}' í´ë”ì— ìŒì„± íŒŒì¼ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    total = len(vocab_list)

    for i, item in enumerate(vocab_list):
        lemma = item.get("lemma")
        script_text = item.get("koChirpScript")

        if not lemma or not script_text:
            print(f"[{i+1}/{total}] ê±´ë„ˆëœ€: lemma ë˜ëŠ” koChirpScript ì—†ìŒ.")
            continue

        # âœ… ì´ í•­ëª© ì „ì²´ì— ì ìš©í•  ì„±ë³„ ê²°ì • (êµëŒ€)
        gender = gender_for_index(i, START_GENDER)
        voice_map = VOICE_SETS[gender]
        print(f"[{i+1}/{total}] '{lemma}' â†’ {gender} ë³´ì´ìŠ¤")

        merged_audio = AudioSegment.empty()
        for lang_code, seg_text in split_script_by_language(script_text):
            if not seg_text or not lang_code:
                continue

            voice = voice_map.get(lang_code)
            if not voice:
                # ì–¸ì–´ ë§¤í•‘ ëˆ„ë½ ì‹œ í•œêµ­ì–´ ë³´ì´ìŠ¤ë¡œ í´ë°±
                voice = voice_map.get("en-US")

            try:
                synthesis_input = texttospeech.SynthesisInput(text=seg_text)
                resp = tts_client.synthesize_speech(
                    input=synthesis_input, voice=voice, audio_config=audio_config
                )
                audio_part = AudioSegment.from_file(BytesIO(resp.audio_content), format="mp3")
                merged_audio += audio_part
            except Exception as e:
                print(f"âŒ í•©ì„± ì˜¤ë¥˜: '{seg_text}' ({gender}/{lang_code}) â†’ {e}")

        if len(merged_audio) > 0:
            # âœ… STT í’ˆì§ˆ ê²Œì´íŠ¸: í†µê³¼ ì‹œì—ë§Œ ì €ì¥
            ok, sim, avg_conf, transcript = _stt_passes(merged_audio, script_text, stt_client)
            if ok:
                out_path = os.path.join(output_dir, f"{sanitize_filename(lemma)}.mp3")
                merged_audio.export(out_path, format="mp3")
                conf_str = f"{avg_conf:.2f}" if avg_conf is not None else "n/a"
                print(f"âœ… PASS  sim={sim:.2%}, stt_conf={conf_str} â†’ ì €ì¥: {out_path}")
            else:
                conf_str = f"{avg_conf:.2f}" if avg_conf is not None else "n/a"
                print(f"ğŸ—‘ï¸ FAIL  sim={sim:.2%}, stt_conf={conf_str} â†’ ë¯¸ì €ì¥: '{lemma}'")
        else:
            print(f"âš ï¸ '{lemma}'ì— ëŒ€í•´ ìœ íš¨í•œ ìŒì„±ì´ ì—†ìŒ.")

    print(f"\nâœ… ì™„ë£Œ: '{output_dir}' í™•ì¸")

if __name__ == "__main__":
    file_to_process = "ielts_b1_7.json"
    synthesize_vocab_audio(file_to_process)
